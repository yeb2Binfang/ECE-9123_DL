{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:pDL] *",
      "language": "python",
      "name": "conda-env-pDL-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "03-autograd_tutorial.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7E-z9AzoK9e"
      },
      "source": [
        "# Autograd: automatic differentiation\n",
        "\n",
        "The ``autograd`` package provides automatic differentiation for all operations\n",
        "on Tensors. It is a define-by-run framework, which means that your backprop is\n",
        "defined by how your code is run, and that every single iteration can be\n",
        "different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMlJYBCsoK9k"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BOeQZEGoK9l"
      },
      "source": [
        "Create a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp-8zdBXoK9l"
      },
      "source": [
        "# Create a 2x2 tensor with gradient-accumulation capabilities\n",
        "x = torch.tensor([[1, 2], [3, 4]], requires_grad=True, dtype=torch.float32)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hIl8RBioK9m"
      },
      "source": [
        "Do an operation on the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w9NFXzloK9m"
      },
      "source": [
        "# Deduct 2 from all elements\n",
        "y = x - 2\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n_l4jpjoK9n"
      },
      "source": [
        "``y`` was created as a result of an operation, so it has a ``grad_fn``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaNJN0rvoK9n"
      },
      "source": [
        "print(y.grad_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9m5l3QmoK9o"
      },
      "source": [
        "# What's happening here?\n",
        "print(x.grad_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3qOqXnxoK9o"
      },
      "source": [
        "# Let's dig further...\n",
        "y.grad_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQwUTwjroK9q"
      },
      "source": [
        "y.grad_fn.next_functions[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thzqk10goK9r"
      },
      "source": [
        "y.grad_fn.next_functions[0][0].variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vW2vNl4oK9r"
      },
      "source": [
        "# Do more operations on y\n",
        "z = y * y * 3\n",
        "a = z.mean()  # average\n",
        "\n",
        "print(z)\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUWSEpbxoK9r"
      },
      "source": [
        "# Let's visualise the computational graph! (thks @szagoruyko)\n",
        "from torchviz import make_dot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY3r-0BEoK9s"
      },
      "source": [
        "make_dot(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94fabj5EoK9s"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "Let's backprop now `out.backward()` is equivalent to doing `out.backward(torch.tensor([1.0]))`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP4ZijPNoK9s"
      },
      "source": [
        "# Backprop\n",
        "a.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeIU4BvNoK9t"
      },
      "source": [
        "Print gradients $\\frac{\\text{d}a}{\\text{d}x}$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOj0e4tloK9t"
      },
      "source": [
        "# Compute it by hand BEFORE executing this\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfSxqwctoK9t"
      },
      "source": [
        "You can do many crazy things with autograd!\n",
        "> With Great *Flexibility* Comes Great Responsibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Trysf_loK9u"
      },
      "source": [
        "# Dynamic graphs!\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "i = 0\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "    i += 1\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "601P9pkaoK9u"
      },
      "source": [
        "# If we don't run backward on a scalar we need to specify the grad_output\n",
        "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
        "y.backward(gradients)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAbNcN70oK9u"
      },
      "source": [
        "# BEFORE executing this, can you tell what would you expect it to print?\n",
        "print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWDEyCaaoK9v"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fi8jWmLoK9v"
      },
      "source": [
        "# This variable decides the tensor's range below\n",
        "n = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhnBFvMyoK9v"
      },
      "source": [
        "# Both x and w that allows gradient accumulation\n",
        "x = torch.arange(1., n + 1, requires_grad=True)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "z = w @ x\n",
        "z.backward()\n",
        "print(x.grad, w.grad, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljCC0ciIoK9v"
      },
      "source": [
        "# Only w that allows gradient accumulation\n",
        "x = torch.arange(1., n + 1)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "z = w @ x\n",
        "z.backward()\n",
        "print(x.grad, w.grad, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9D1syGGoK9w"
      },
      "source": [
        "x = torch.arange(1., n + 1)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "\n",
        "# Regardless of what you do in this context, all torch tensors will not have gradient accumulation\n",
        "with torch.no_grad():\n",
        "    z = w @ x\n",
        "\n",
        "try:\n",
        "    z.backward()  # PyTorch will throw an error here, since z has no grad accum.\n",
        "except RuntimeError as e:\n",
        "    print('RuntimeError!!! >:[')\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqTaATnAoK9w"
      },
      "source": [
        "## More stuff\n",
        "\n",
        "Documentation of the automatic differentiation package is at\n",
        "http://pytorch.org/docs/autograd."
      ]
    }
  ]
}