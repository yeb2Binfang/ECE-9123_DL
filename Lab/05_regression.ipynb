{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:dl-minicourse] *",
      "language": "python",
      "name": "conda-env-dl-minicourse-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "05-regression.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NME-ExeGoMBo"
      },
      "source": [
        "# Regression\n",
        "## Create the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUPSJhkzoMBx"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import math\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkcMPYnCoMCI"
      },
      "source": [
        "from res.plot_lib import plot_data, plot_model, set_default\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQoSucuIoMCI"
      },
      "source": [
        "set_default()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy7vHtgBoMCJ"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRrJsw_RoMCJ"
      },
      "source": [
        "seed = 1\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "N = 1000  # num_samples_per_class\n",
        "D = 1  # dimensions\n",
        "C = 1  # num_classes\n",
        "H = 100  # num_hidden_units"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9pG2F6OoMCJ"
      },
      "source": [
        "X = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1).to(device)\n",
        "y = X.pow(3) + 0.3 * torch.rand(X.size()).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoxg8U5ooMCK"
      },
      "source": [
        "print(\"Shapes:\")\n",
        "print(\"X:\", tuple(X.size()))\n",
        "print(\"y:\", tuple(y.size()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzmpbrgyoMCK"
      },
      "source": [
        "plt.scatter(X.cpu().numpy(), y.cpu().numpy())\n",
        "plt.axis('equal');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y93nrCloMCK"
      },
      "source": [
        "## Linear model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb86DR6IoMCL"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "lambda_l2 = 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-sKmPdnoMCL"
      },
      "source": [
        "# nn package to create our linear model\n",
        "# each Linear module has a weight and bias\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(D, H),\n",
        "    nn.Linear(H, C)\n",
        ")\n",
        "model.to(device) # Convert to CUDA\n",
        "\n",
        "# nn package also has different loss functions.\n",
        "# we use MSE loss for our regression task\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# we use the optim package to apply\n",
        "# stochastic gradient descent for our parameter updates\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
        "\n",
        "# Training\n",
        "for t in range(1000):\n",
        "    \n",
        "    # Feed forward to get the logits\n",
        "    y_pred = model(X)\n",
        "    \n",
        "    # Compute the loss (MSE)\n",
        "    loss = criterion(y_pred, y)\n",
        "    print(\"[EPOCH]: %i, [LOSS or MSE]: %.6f\" % (t, loss.item()))\n",
        "    display.clear_output(wait=True)\n",
        "    \n",
        "    # zero the gradients before running\n",
        "    # the backward pass.\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Backward pass to compute the gradient\n",
        "    # of loss w.r.t our learnable params. \n",
        "    loss.backward()\n",
        "    \n",
        "    # Update params\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffmczb5voMCL"
      },
      "source": [
        "# Plot trained model\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPnYRCj2oMCM"
      },
      "source": [
        "plt.scatter(X.data.cpu().numpy(), y.data.cpu().numpy())\n",
        "plt.plot(X.data.cpu().numpy(), y_pred.data.cpu().numpy(), 'r-', lw=5)\n",
        "plt.axis('equal');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZAqtS39oMCM"
      },
      "source": [
        "## Two-layered network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC5pyb0voMCM"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "lambda_l2 = 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "60e-ANlaoMCM"
      },
      "source": [
        "# Number of networks\n",
        "n_networks = 10\n",
        "models = list()\n",
        "y_pretrain = list()\n",
        "\n",
        "# nn package also has different loss functions.\n",
        "# we use MSE for a regression task\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "for mod in range(n_networks):\n",
        "    # nn package to create our linear model\n",
        "    # each Linear module has a weight and bias\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(D, H),\n",
        "        nn.ReLU() if mod < n_networks // 2 else nn.Tanh(),\n",
        "        nn.Linear(H, C)\n",
        "    )\n",
        "    model.to(device)\n",
        "    \n",
        "    # Append models\n",
        "    models.append(model)\n",
        "\n",
        "    # we use the optim package to apply\n",
        "    # ADAM for our parameter updates\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
        "\n",
        "    # e = 1.  # plotting purpose\n",
        "\n",
        "    # Training\n",
        "    for t in range(1000):\n",
        "\n",
        "        # Feed forward to get the logits\n",
        "        y_pred = model(X)\n",
        "        \n",
        "        # Append pre-train output\n",
        "        if t == 0:\n",
        "            y_pretrain.append(y_pred.detach())\n",
        "\n",
        "        # Compute the loss and accuracy\n",
        "        loss = criterion(y_pred, y)\n",
        "        print(f\"[MODEL]: {mod + 1}, [EPOCH]: {t}, [LOSS]: {loss.item():.6f}\")\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "        # zero the gradients before running\n",
        "        # the backward pass.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass to compute the gradient\n",
        "        # of loss w.r.t our learnable params. \n",
        "        loss.backward()\n",
        "\n",
        "        # Update params\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H5zHeZ8oMCN"
      },
      "source": [
        "print(models[0], models[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlnnrQWjoMCN"
      },
      "source": [
        "## Predictions: Before Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f85gpWi6oMCN"
      },
      "source": [
        "for y_pretrain_idx in y_pretrain:\n",
        "    # New X that ranges from -5 to 5 instead of -1 to 1\n",
        "    X_new = torch.unsqueeze(torch.linspace(-2, 2, 100), dim=1)\n",
        "        \n",
        "    plt.plot(X_new.numpy(), y_pretrain_idx.cpu().numpy(), 'r-', lw=1)\n",
        "\n",
        "plt.scatter(X.cpu().numpy(), y.cpu().numpy(), label='data')\n",
        "plt.axis('square')\n",
        "plt.axis((-1.1, 1.1, -1.1, 1.1));\n",
        "y_combo = torch.stack(y_pretrain)\n",
        "plt.plot(X_new.numpy(), y_combo.var(dim=0).cpu().numpy(), 'g', label='variance');\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpZu0FxYoMCN"
      },
      "source": [
        "## Predictions: After Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A-Z3IHMoMCO"
      },
      "source": [
        "y_pred = list()\n",
        "relu_models = models[:n_networks // 2]\n",
        "tanh_models = models[n_networks // 2:]\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "def dense_prediction(models, non_linearity, zoom):\n",
        "    plt.subplot(1, 2, 1 if non_linearity == 'ReLU' else 2)\n",
        "    for model in models:\n",
        "        # New X that ranges from -5 to 5 instead of -1 to 1\n",
        "        X_new = torch.unsqueeze(torch.linspace(-4, 4, 1001), dim=1).to(device)\n",
        "\n",
        "        # Getting predictions from input\n",
        "        with torch.no_grad():\n",
        "            y_pred.append(model(X_new))\n",
        "\n",
        "        plt.plot(X_new.cpu().numpy(), y_pred[-1].cpu().numpy(), 'r-', lw=1)\n",
        "    plt.scatter(X.cpu().numpy(), y.cpu().numpy(), label='data')\n",
        "    plt.axis('square')\n",
        "    plt.axis(torch.tensor((-1.1, 1.1, -1.1, 1.1)) * zoom);\n",
        "    y_combo = torch.stack(y_pred)\n",
        "    plt.plot(X_new.cpu().numpy(), 10 * y_combo.var(dim=0).cpu().sqrt().numpy(), 'y', label='10 × std')\n",
        "    plt.plot(X_new.cpu().numpy(), 10 * y_combo.var(dim=0).cpu().numpy(), 'g', label='30 × variance')\n",
        "    plt.legend()\n",
        "    plt.title(non_linearity + ' models')\n",
        "\n",
        "z = 1  # try 1 or 4\n",
        "dense_prediction(relu_models, 'ReLU', zoom=z)\n",
        "dense_prediction(tanh_models, 'Tanh', zoom=z)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}